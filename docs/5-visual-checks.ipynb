{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this very short chapter, we will introduce basic visual checks that you can easily write for your data.\n",
    "You could consider this to be the collection of non-automatable, human-in-the-loop pieces that one needs to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visual Diagnostics\n",
    "\n",
    "Now that we're done with the schema checks, let's do some sanity checks on the data as well. This is my personal favourite too, as some of the activities here overlap with the early stages of exploratory data analysis.\n",
    "\n",
    "We're going to switch datasets here, and move to a 'corrupted' version of the Boston Economic Indicators dataset. Its file path is: `./data/boston_ei-corrupt.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/boston_ei-corrupt.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demo: Visual Diagnostics\n",
    "\n",
    "We can use a package called `missingno`, which gives us a quick visual view of the completeness of the data. This is a good starting point for deciding whether you need to manually comb through the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First, we check for missing data.\n",
    "import missingno as msno\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Immediately it's clear that there's a number of rows with empty values! Nothing beats a quick visual check like this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can get a table version of this using another package called `pandas_summary`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can do the same using pandas-summary.\n",
    "from pandas_summary import DataFrameSummary\n",
    "\n",
    "dfs = DataFrameSummary(df)\n",
    "dfs.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`dfs.summary()` returns a Pandas DataFrame; this means we can write tests for data completeness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distributions\n",
    "\n",
    "Most of what is coming is going to be a demonstration of the kinds of tools that are potentially useful for you. Feel free to relax from coding, as these aren't necessarily obviously automatable.\n",
    "\n",
    "### Numerical Data\n",
    "\n",
    "We can take the EDA portion further, by doing an empirical cumulative distribution plot for each data column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_dimensions(length):\n",
    "    \"\"\"\n",
    "    Given an integer, compute the \"square-est\" pair of dimensions for plotting.\n",
    "    \n",
    "    Examples:\n",
    "    - length: 17 => rows: 4, cols: 5\n",
    "    - length: 14 => rows: 4, cols: 4\n",
    "    \n",
    "    This is a utility function; can be tested separately.\n",
    "    \"\"\"\n",
    "    sqrt = np.sqrt(length)\n",
    "    floor = int(np.floor(sqrt))\n",
    "    ceil = int(np.ceil(sqrt))\n",
    "    \n",
    "    if floor ** 2 >= length:\n",
    "        return (floor, floor)\n",
    "    elif floor * ceil >= length:\n",
    "        return (floor, ceil)\n",
    "    else:\n",
    "        return (ceil, ceil)\n",
    "    \n",
    "compute_dimensions(length=17)\n",
    "\n",
    "assert compute_dimensions(17) == (4, 5)\n",
    "assert compute_dimensions(16) == (4, 4)\n",
    "assert compute_dimensions(15) == (4, 4)\n",
    "assert compute_dimensions(11) == (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Next, let's visualize the empirical CDF for each column of data.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def empirical_cumdist(data, ax, title=None):\n",
    "    \"\"\"\n",
    "    Plots the empirical cumulative distribution of values.\n",
    "    \"\"\"\n",
    "    x, y = np.sort(data), np.arange(1, len(data)+1) / len(data)\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "data_cols = [i for i in df.columns if i not in ['Year', 'Month']]\n",
    "n_rows, n_cols = compute_dimensions(len(data_cols))\n",
    "\n",
    "fig = plt.figure(figsize=(n_cols*3, n_rows*3))\n",
    "from matplotlib.gridspec import GridSpec\n",
    "gs = GridSpec(n_rows, n_cols)\n",
    "for i, col in enumerate(data_cols):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    empirical_cumdist(df[col], ax, title=col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's often a good idea to **standardize** numerical data (that aren't count data). The term **standardize** often refers to the statistical procedure of subtracting the mean and dividing by the standard deviation, yielding an empirical distribution of data centered on 0 and having standard deviation of 1.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write a test for a function that standardizes a column of data. Then, write the function.\n",
    "\n",
    "**Note:** This function is also implemented in the `scikit-learn` library as part of their `preprocessing` module. However, in case an engineering decision that you make is that you don't want to import an entire library just to use one function, you can re-implement it on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def standard_scaler(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "def test_standard_scaler(x):\n",
    "    std = standard_scaler(x)\n",
    "    assert np.allclose(std.mean(), 0)\n",
    "    assert np.allclose(std.std(), 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now, plot the grid of standardized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [i for i in df.columns if i not in ['Year', 'Month']]\n",
    "n_rows, n_cols = compute_dimensions(len(data_cols))\n",
    "\n",
    "fig = plt.figure(figsize=(n_cols*3, n_rows*3))\n",
    "from matplotlib.gridspec import GridSpec\n",
    "gs = GridSpec(n_rows, n_cols)\n",
    "for i, col in enumerate(data_cols):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    empirical_cumdist(standard_scaler(df[col]), ax, title=col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Did we just copy/paste the function?! It's time to stop doing this. Let's refactor the code into a function that can be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical Data\n",
    "\n",
    "For categorical-type data, we can plot the empirical distribution as well. (This example uses the `smartphone_sanitization.csv` dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def empirical_catdist(data, ax, title=None):\n",
    "    d = Counter(data)\n",
    "    print(d)\n",
    "    x = range(len(d.keys()))\n",
    "    labels = list(d.keys())\n",
    "    y = list(d.values())\n",
    "    ax.bar(x, y)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "smartphone_df = pd.read_csv('data/smartphone_sanitization.csv')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "empirical_catdist(smartphone_df['site'], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Checks\n",
    "\n",
    "- Report on deviations from normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normality?!\n",
    "\n",
    "- The Gaussian (Normal) distribution is commonly assumed in downstream statistical procedures, e.g. outlier detection.\n",
    "- We can test for normality by using a K-S test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-S test\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "> In statistics, the Kolmogorov–Smirnov test (K–S test or KS test) is a nonparametric test of the equality of continuous, one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test). It is named after Andrey Kolmogorov and Nikolai Smirnov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/c/cf/KS_Example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "import numpy.random as npr\n",
    "\n",
    "# Simulate a normal distribution with 10000 draws.\n",
    "normal_rvs = npr.normal(size=10000)\n",
    "result = ks_2samp(normal_rvs, df['labor_force_part_rate'].dropna())\n",
    "result.pvalue < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "empirical_cumdist(normal_rvs, ax=ax)\n",
    "empirical_cumdist(df['hotel_occup_rate'], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Re-create the panel of cumulative distribution plots, this time adding on the Normal distribution, and annotating the p-value of the K-S test in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_cols = [i for i in df.columns if i not in ['Year', 'Month']]\n",
    "n_rows, n_cols = compute_dimensions(len(data_cols))\n",
    "\n",
    "fig = plt.figure(figsize=(n_cols*3, n_rows*3))\n",
    "from matplotlib.gridspec import GridSpec\n",
    "gs = GridSpec(n_rows, n_cols)\n",
    "for i, col in enumerate(data_cols):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    test = ks_2samp(normal_rvs, standard_scaler(df[col]))\n",
    "    empirical_cumdist(normal_rvs, ax)\n",
    "    empirical_cumdist(standard_scaler(df[col]), ax, title=f\"{col}, p={round(test.pvalue, 2)}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datatest",
   "language": "python",
   "name": "datatest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
