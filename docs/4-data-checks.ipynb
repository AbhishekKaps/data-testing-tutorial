{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this chapter, we are going to explore how you can structure your data processing code, such that they are aware of what \"valid\" inputs look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Validation\n",
    "\n",
    "The key idea we will be introducing here is \"schema validation\".\n",
    "\n",
    "Before we go on, though, we should clarify some terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "\n",
    "Firstly, what's a **schema**? From the [Wikipedia entry on database schemas](https://en.wikipedia.org/wiki/Database_schema):\n",
    "\n",
    "> The database schema of a database is its structure described in a formal language supported by the database management system (DBMS). The term \"schema\" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of relational databases).\n",
    "\n",
    "For the purpose of this handbook, a schema can be described as:\n",
    "\n",
    "> A declaration of what columns are expected to exist in a table: their names, data types, and valid ranges for each entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Validation\n",
    "\n",
    "Next up, what about **schema validation**? Schema validation here refers to the act of checking that our data matches the schema that it is being checked against. This checking can be done for the input to a function and for the function's output too.\n",
    "\n",
    "In other words, in our data processing functions, we check that the dataframe that we're handling follows the declared assumptions of what our data _ought_ to look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When do we do schema validation?\n",
    "\n",
    "Here are a few situations where schema validation is an important thing to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the data collection phase\n",
    "\n",
    "If you're lucky, at the data collection phase, schema validation is already being performed, regardless of whether your data are being written to log files, added directly into a SQL database, or written in batches as parquet files. If your data are being written directly to a SQL database, then schema validation is already being performed, to the degree of stringency that the tables' columns are being specified!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the data transformation phase\n",
    "\n",
    "If you're still lucky, a team of data engineers is helping you structure your data upstream of your use for analysis. Usually this means the data are being stored in a database, which has a SQL schema attached to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the analysis phase\n",
    "\n",
    "Regardless of whether or not your data are being schema-fied upstream to when it gets to you, you should definitely take the time to walk carefully through the data and declare the expected schema that you need for your Python-based data processing functions.\n",
    "\n",
    "This is the phase where you, the data scientist, can take action. In particular, I am going to show you the basics of how to declare pandas dataframe schemas using the Python package called `pandera`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame schemas with `pandera`\n",
    "\n",
    "`pandera` is statistical validation package for pandas dataframes. Compared to alternatives that I have seen, `pandera` looks the most \"natural\" in its usage idioms. It is also very lightweight, focuses on doing one and only one thing well, and is very easily incorporated into a wide variety of data processing workflows.\n",
    "\n",
    "Nothing beats learning by doing. As such, to help illustrate how to use `pandera`, I've designed an example that you can walk through to get a feel for how to use `pandera`. After that, there are a few more exercises that you can use to get more practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Declaring schemas with `pandera`\n",
    "\n",
    "At the heart of `pandera` is the declaration of a dataframe schema.\n",
    "\n",
    "Here is a plain text description of a dataframe we will be working with for this exercise, and its corresponding dataframe schema.\n",
    "\n",
    "> Our dataframe has three columns:\n",
    "> - `name`: a string column, with no nulls expected,\n",
    "> - `age`: an integer column, positive only, nulls possibly present, but required for calculations,\n",
    "> - `home_province`: a string column, and should be one of the 10 provinces or 3 territories of Canada, in its abbreviated form. Should not be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandera import Column, DataFrameSchema, Check\n",
    "import pandera as pa\n",
    "\n",
    "valid_provinces = [\"AB\", \"BC\", \"MB\", \"NB\", \"NL\", \"NT\", \"NS\", \"NU\", \"ON\", \"PE\", \"QC\", \"SK\", \"YK\"]\n",
    "\n",
    "canadians_schema = DataFrameSchema(\n",
    "    columns={\n",
    "        \"name\": Column(pa.String, nullable=False),\n",
    "        \"age\": Column(pa.Int, checks=[Check.greater_than(0)], nullable=True, required=True),\n",
    "        \"home_province\": Column(pa.String, checks=[Check.isin(valid_provinces)], nullable=False)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say we have a function that accepts data of a bunch of Canadians,\n",
    "and tries to calculate the mean age of individuals per province:\n",
    "\n",
    "```python\n",
    "def mean_age(df):\n",
    "    return df.groupby(\"home_province\").mean()\n",
    "```\n",
    "\n",
    "This function is one that we might consider \"brittle\", because there are no checks in the function that encode what we might expect to see for the dataframe `df`.\n",
    "\n",
    "However, with `pandera`'s `check_input` decorator, we can validate the input data against the `canadians_schema` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandera import check_input\n",
    "import pandas as pd\n",
    "\n",
    "@check_input(schema=canadians_schema)\n",
    "def mean_age(df):\n",
    "    return df.groupby(\"home_province\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then when fed _invalid_ Canadian data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaError",
     "evalue": "error in check_input decorator of function 'mean_age': <Schema Column: 'age' type=int> failed element-wise validator 0:\n<Check _greater_than: greater_than(0)>\nfailure cases:\n   index  failure_case\n0      0             0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/decorators.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchemaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/schemas.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchemaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                 \u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schema_component_check\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/error_handlers.py\u001b[0m in \u001b[0;36mcollect_error\u001b[0;34m(self, reason_code, schema_error, original_exc)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mschema_error\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moriginal_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/schemas.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 check_results.append(isinstance(\n\u001b[0;32m--> 413\u001b[0;31m                     schema_component(dataframe_to_validate), pd.DataFrame))\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchemaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/schemas.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;34m\"\"\"Alias for ``validate`` method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/schema_components.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 isinstance(\n\u001b[0;32m--> 169\u001b[0;31m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     .validate(\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/schemas.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchemaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m                 \u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataframe_check\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/error_handlers.py\u001b[0m in \u001b[0;36mcollect_error\u001b[0;34m(self, reason_code, schema_error, original_exc)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mschema_error\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moriginal_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/schemas.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 check_results.append(\n\u001b[0;32m--> 919\u001b[0;31m                     _handle_check_results(\n\u001b[0m\u001b[1;32m    920\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcheck_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/schemas.py\u001b[0m in \u001b[0;36m_handle_check_results\u001b[0;34m(schema, check_index, check, check_obj, *check_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         raise errors.SchemaError(\n\u001b[0m\u001b[1;32m   1097\u001b[0m             \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSchemaError\u001b[0m: <Schema Column: 'age' type=int> failed element-wise validator 0:\n<Check _greater_than: greater_than(0)>\nfailure cases:\n   index  failure_case\n0      0             0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c73492a233b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m })\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmean_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanadians_bad_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/datatest/lib/python3.8/site-packages/pandera/decorators.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 )\n\u001b[0;32m--> 151\u001b[0;31m                 raise errors.SchemaError(\n\u001b[0m\u001b[1;32m    152\u001b[0m                     \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0mfailure_cases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailure_cases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSchemaError\u001b[0m: error in check_input decorator of function 'mean_age': <Schema Column: 'age' type=int> failed element-wise validator 0:\n<Check _greater_than: greater_than(0)>\nfailure cases:\n   index  failure_case\n0      0             0"
     ]
    }
   ],
   "source": [
    "canadians_bad_data = pd.DataFrame({\n",
    "    \"age\": [0, 10, 20, 30],\n",
    "    \"name\": [\"adrian\", \"becky\", \"charlie\", \"david\"],\n",
    "    \"home_province\": [\"AB\", \"BC\", \"PEI\", \"NU\"]\n",
    "})\n",
    "\n",
    "mean_age(canadians_bad_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Voila!_ `pandera` told us exactly which row in the dataframe failed the schema check.\n",
    "If this is the responsibility of our data provider,\n",
    "we can now go and complain to them that the dataframe has errors,\n",
    "and show them the error message.\n",
    "\n",
    "On the other hand, if you have good data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_province</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AB</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NU</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PE</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age\n",
       "home_province     \n",
       "AB               5\n",
       "BC              10\n",
       "NU              30\n",
       "PE              20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canadians_good_data = pd.DataFrame({\n",
    "    \"age\": [5, 10, 20, 30],\n",
    "    \"name\": [\"adrian\", \"becky\", \"charlie\", \"david\"],\n",
    "    \"home_province\": [\"AB\", \"BC\", \"PE\", \"NU\"]\n",
    "})\n",
    "\n",
    "mean_age(canadians_good_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandera` doesn't error out, and instead lets the calculation proceed as per normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandera` usage tips\n",
    "\n",
    "Here's some tips that I have for using `pandera`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store schemas in its own `.py` file\n",
    "\n",
    "I usually store them in `src/data/schemas.py`, so that the Python object is importable later on, and I can use it anywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the default checks when possible\n",
    "\n",
    "`pandera` comes with a [rich library of built-in checks](https://pandera.readthedocs.io/en/stable/generated/pandera.Check.html#pandera.Check). I'd recommend using these wherever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom checks can be implemented\n",
    "\n",
    "If you so choose to, your custom checks can be implemented easily by writing a custom function that takes in a pandas Series and returns a boolean Series or a single boolean. For example, `Check.greater_than(0)` can also be expressed as `lambda s: s > 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises with `pandera`\n",
    "\n",
    "Now, it's your turn to get practice writing `pandera` schemas. Give it a shot using the following datasets and data preprocessing functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Boston budget summarization\n",
    "\n",
    "In this exercise, we are going to summarize data from the City of Boston's budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To get the columns of a DataFrame object `df`, call `df.columns`. This is a list-like object that can be iterated over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## YAML Files\n",
    "\n",
    "Describe data in a human-friendly & computer-readable format. The `environment.yml` file in your downloaded repository is also a YAML file, by the way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Structure:\n",
    "\n",
    "```yaml\n",
    "key1: value\n",
    "key2:\n",
    "- value1\n",
    "- value2\n",
    "- subkey1:\n",
    "    - value3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example YAML-formatted schema:\n",
    "\n",
    "```yaml\n",
    "filename: boston_budget.csv\n",
    "column_names:\n",
    "- \"Fiscal Year\"\n",
    "- \"Service (cabinet)\"\n",
    "- \"Department\"\n",
    "- \"Program #\"\n",
    "...\n",
    "- \"Fund\"\n",
    "- \"Amount\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "YAML-formatted text can be read as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spec = \"\"\"\n",
    "filename: boston_budget.csv\n",
    "columns:\n",
    "- \"Fiscal Year\"\n",
    "- \"Service (Cabinet)\"\n",
    "- \"Department\"\n",
    "- \"Program #\"\n",
    "- \"Program\"\n",
    "- \"Expense Type\"\n",
    "- \"ACCT #\"\n",
    "- \"Expense Category (Account)\"\n",
    "- \"Fund\"\n",
    "- \"Amount\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "metadata = yaml.load(spec)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take dictionaries, and return YAML-formatted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml.dump(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By having things YAML formatted, you preserve human-readability and computer-readability simultaneously. \n",
    "\n",
    "Providing metadata should be something already done when doing analytics; YAML-format is a strong suggestion, but YAML schema will depend on use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now switch roles, and pretend that we're on side of the \"analyst\" and are no longer the \"data provider\". \n",
    "\n",
    "How would you check that the columns match the spec? Basically, check that every element in `df.columns` is present inside the `metadata['columns']` list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Inside `test_datafuncs.py`, write a utility function, `check_schema(df, meta_columns)` that tests whether every column in a DataFrame is present in some metadata spec file. It should accept two arguments:\n",
    "\n",
    "- `df`: a `pandas.DataFrame`\n",
    "- `meta_columns`: A list of columns from the metadata spec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def check_schema(df, meta_columns):\n",
    "    for col in df.columns:\n",
    "        assert col in meta_columns, f'\"{col}\" not in metadata column spec'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your test file, outside the function definition, write another test function, `test_budget_schemas()`, explicitly runs a test for just the budget data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def test_budget_schemas():\n",
    "    columns = read_metadata('data/metadata_budget.yml')['columns']\n",
    "    df = pd.read_csv('data/boston_budget.csv')\n",
    "\n",
    "    check_schema(df, columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the test. Do you get the following error? Can you spot the error?\n",
    "\n",
    "```bash\n",
    "    def check_schema(df, meta_columns):\n",
    "        for col in df.columns:\n",
    ">           assert col in meta_columns, f'\"{col}\" not in metadata column spec'\n",
    "E           AssertionError: \" Amount\" not in metadata column spec\n",
    "E           assert ' Amount' in ['Fiscal Year', 'Service (Cabinet)', 'Department', 'Program #', 'Program', 'Expense Type', ...]\n",
    "\n",
    "test_datafuncs_soln.py:63: AssertionError\n",
    "=================================== 1 failed, 7 passed in 0.91 seconds ===================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If there is even a slight mis-spelling, this kind of check will help you pinpoint where that is. Note how the \"Amount\" column is spelled with an extra space. \n",
    "\n",
    "At this point, I would contact the data provider to correct errors like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is a logical practice to keep one schema spec file per table provided to you. However, it is also possible to take advantage of YAML \"documents\" to keep multiple schema specs inside a single YAML file. \n",
    "\n",
    "The choice is yours - in cases where there are a lot of data files, it may make sense (for the sake of file-system sanity) to keep all of the specs in multiple files that represent logical groupings of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: Write `YAML` metadata spec.\n",
    "\n",
    "Put yourself in the shoes of a data provider. Take the `boston_ei.csv` file in the `data/` directory, and make a schema spec file for that file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: Write test for metadata spec.\n",
    "\n",
    "Next, put yourself in the shoes of a data analyst. Take the schema spec file and write a test for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Auto YAML Spec.\n",
    "\n",
    "Inside `datafuncs.py`, write a function with the signature `autospec(handle)` that takes in a file path, and does the following:\n",
    "\n",
    "- Create a dictionary, with two keys:\n",
    "    - a \"filename\" key, whose value only records the filename (and not the full file path),\n",
    "    - a \"columns\" key, whose value records the list of columns in the dataframe.\n",
    "- Converts the dictionary to a YAML string\n",
    "- Writes the YAML string to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optional Exercise: Write meta-test\n",
    "\n",
    "Now, let's go \"meta\". Write a \"meta-test\" that ensures that every CSV file in the `data/` directory has a schema file associated with it. (The function need not check each schema.) Until we finish filling out the rest of the exercises, this test can be allowed to fail, and we can mark it as a test to skip by marking it with an `@skip` decorator:\n",
    "\n",
    "```python\n",
    "@pytest.mark.skip(reason=\"no way of currently testing this\")\n",
    "def test_my_func():\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Notes\n",
    "\n",
    "- The point here is to have a trusted copy of schema apart from data file. YAML not necessarily only way!\n",
    "- If no schema provided, manually create one; this is exploratory data analysis anyways - no effort wasted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Datum Checks\n",
    "\n",
    "Now that we're done with the schema checks, let's do some sanity checks on the data as well. This is my personal favourite too, as some of the activities here overlap with the early stages of exploratory data analysis.\n",
    "\n",
    "We're going to switch datasets here, and move to a 'corrupted' version of the Boston Economic Indicators dataset. Its file path is: `./data/boston_ei-corrupt.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/boston_ei-corrupt.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demo: Visual Diagnostics\n",
    "\n",
    "We can use a package called `missingno`, which gives us a quick visual view of the completeness of the data. This is a good starting point for deciding whether you need to manually comb through the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First, we check for missing data.\n",
    "import missingno as msno\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Immediately it's clear that there's a number of rows with empty values! Nothing beats a quick visual check like this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can get a table version of this using another package called `pandas_summary`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can do the same using pandas-summary.\n",
    "from pandas_summary import DataFrameSummary\n",
    "\n",
    "dfs = DataFrameSummary(df)\n",
    "dfs.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`dfs.summary()` returns a Pandas DataFrame; this means we can write tests for data completeness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: Test for data completeness.\n",
    "\n",
    "Write a test named `check_data_completeness(df)` that takes in a DataFrame and confirms that there's no missing data from the `pandas-summary` output. Then, write a corresponding `test_boston_ei()` that tests the schema for the Boston Economic Indicators dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# In test_datafuncs.py\n",
    "from pandas_summary import DataFrameSummary\n",
    "def check_data_completeness(df):\n",
    "    \n",
    "    df_summary = DataFrameSummary(df).summary()\n",
    "    for col in df_summary.columns:\n",
    "        assert df_summary.loc['missing', col] == 0, f'{col} has missing values'\n",
    "        \n",
    "def test_boston_ei():\n",
    "    df = pd.read_csv('data/boston_ei.csv')\n",
    "    check_data_completeness(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: Test for value correctness.\n",
    "\n",
    "In the Economic Indicators dataset, there are four \"rate\" columns: `['labor_force_part_rate', 'hotel_occup_rate', 'hotel_avg_daily_rate', 'unemp_rate']`, which must have values between 0 and 1.\n",
    "\n",
    "Add a utility function to `test_datafuncs.py`, `check_data_range(data, lower=0, upper=1)`, which checks the range of the data such that:\n",
    "- `data` is a list-like object.\n",
    "- `data <= upper`\n",
    "- `data >= lower`\n",
    "- `upper` and `lower` have default values of 1 and 0 respectively.\n",
    "\n",
    "Then, add to the `test_boston_ei()` function tests for each of these four columns, using the `check_data_range()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# In test_datafuncs.py\n",
    "def check_data_range(data, lower=0, upper=1):\n",
    "    assert min(data) >= lower, f\"minimum value less than {lower}\"\n",
    "    assert max(data) <= upper, f\"maximum value greater than {upper}\"\n",
    "\n",
    "def test_boston_ei():\n",
    "    df = pd.read_csv('data/boston_ei.csv')\n",
    "    check_data_completeness(df)\n",
    "\n",
    "    zero_one_cols = ['labor_force_part_rate', 'hotel_occup_rate',\n",
    "                     'hotel_avg_daily_rate', 'unemp_rate']\n",
    "    for col in zero_one_cols:\n",
    "        check_data_range(df['labor_force_part_rate'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distributions\n",
    "\n",
    "Most of what is coming is going to be a demonstration of the kinds of tools that are potentially useful for you. Feel free to relax from coding, as these aren't necessarily obviously automatable.\n",
    "\n",
    "### Numerical Data\n",
    "\n",
    "We can take the EDA portion further, by doing an empirical cumulative distribution plot for each data column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_dimensions(length):\n",
    "    \"\"\"\n",
    "    Given an integer, compute the \"square-est\" pair of dimensions for plotting.\n",
    "    \n",
    "    Examples:\n",
    "    - length: 17 => rows: 4, cols: 5\n",
    "    - length: 14 => rows: 4, cols: 4\n",
    "    \n",
    "    This is a utility function; can be tested separately.\n",
    "    \"\"\"\n",
    "    sqrt = np.sqrt(length)\n",
    "    floor = int(np.floor(sqrt))\n",
    "    ceil = int(np.ceil(sqrt))\n",
    "    \n",
    "    if floor ** 2 >= length:\n",
    "        return (floor, floor)\n",
    "    elif floor * ceil >= length:\n",
    "        return (floor, ceil)\n",
    "    else:\n",
    "        return (ceil, ceil)\n",
    "    \n",
    "compute_dimensions(length=17)\n",
    "\n",
    "assert compute_dimensions(17) == (4, 5)\n",
    "assert compute_dimensions(16) == (4, 4)\n",
    "assert compute_dimensions(15) == (4, 4)\n",
    "assert compute_dimensions(11) == (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Next, let's visualize the empirical CDF for each column of data.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def empirical_cumdist(data, ax, title=None):\n",
    "    \"\"\"\n",
    "    Plots the empirical cumulative distribution of values.\n",
    "    \"\"\"\n",
    "    x, y = np.sort(data), np.arange(1, len(data)+1) / len(data)\n",
    "    ax.scatter(x, y)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "data_cols = [i for i in df.columns if i not in ['Year', 'Month']]\n",
    "n_rows, n_cols = compute_dimensions(len(data_cols))\n",
    "\n",
    "fig = plt.figure(figsize=(n_cols*3, n_rows*3))\n",
    "from matplotlib.gridspec import GridSpec\n",
    "gs = GridSpec(n_rows, n_cols)\n",
    "for i, col in enumerate(data_cols):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    empirical_cumdist(df[col], ax, title=col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's often a good idea to **standardize** numerical data (that aren't count data). The term **standardize** often refers to the statistical procedure of subtracting the mean and dividing by the standard deviation, yielding an empirical distribution of data centered on 0 and having standard deviation of 1.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write a test for a function that standardizes a column of data. Then, write the function.\n",
    "\n",
    "**Note:** This function is also implemented in the `scikit-learn` library as part of their `preprocessing` module. However, in case an engineering decision that you make is that you don't want to import an entire library just to use one function, you can re-implement it on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def standard_scaler(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "def test_standard_scaler(x):\n",
    "    std = standard_scaler(x)\n",
    "    assert np.allclose(std.mean(), 0)\n",
    "    assert np.allclose(std.std(), 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now, plot the grid of standardized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [i for i in df.columns if i not in ['Year', 'Month']]\n",
    "n_rows, n_cols = compute_dimensions(len(data_cols))\n",
    "\n",
    "fig = plt.figure(figsize=(n_cols*3, n_rows*3))\n",
    "from matplotlib.gridspec import GridSpec\n",
    "gs = GridSpec(n_rows, n_cols)\n",
    "for i, col in enumerate(data_cols):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    empirical_cumdist(standard_scaler(df[col]), ax, title=col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Did we just copy/paste the function?! It's time to stop doing this. Let's refactor the code into a function that can be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical Data\n",
    "\n",
    "For categorical-type data, we can plot the empirical distribution as well. (This example uses the `smartphone_sanitization.csv` dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def empirical_catdist(data, ax, title=None):\n",
    "    d = Counter(data)\n",
    "    print(d)\n",
    "    x = range(len(d.keys()))\n",
    "    labels = list(d.keys())\n",
    "    y = list(d.values())\n",
    "    ax.bar(x, y)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "smartphone_df = pd.read_csv('data/smartphone_sanitization.csv')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "empirical_catdist(smartphone_df['site'], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Checks\n",
    "\n",
    "- Report on deviations from normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normality?!\n",
    "\n",
    "- The Gaussian (Normal) distribution is commonly assumed in downstream statistical procedures, e.g. outlier detection.\n",
    "- We can test for normality by using a K-S test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-S test\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "> In statistics, the Kolmogorov–Smirnov test (K–S test or KS test) is a nonparametric test of the equality of continuous, one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test). It is named after Andrey Kolmogorov and Nikolai Smirnov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/c/cf/KS_Example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "import numpy.random as npr\n",
    "\n",
    "# Simulate a normal distribution with 10000 draws.\n",
    "normal_rvs = npr.normal(size=10000)\n",
    "result = ks_2samp(normal_rvs, df['labor_force_part_rate'].dropna())\n",
    "result.pvalue < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "empirical_cumdist(normal_rvs, ax=ax)\n",
    "empirical_cumdist(df['hotel_occup_rate'], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Re-create the panel of cumulative distribution plots, this time adding on the Normal distribution, and annotating the p-value of the K-S test in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_cols = [i for i in df.columns if i not in ['Year', 'Month']]\n",
    "n_rows, n_cols = compute_dimensions(len(data_cols))\n",
    "\n",
    "fig = plt.figure(figsize=(n_cols*3, n_rows*3))\n",
    "from matplotlib.gridspec import GridSpec\n",
    "gs = GridSpec(n_rows, n_cols)\n",
    "for i, col in enumerate(data_cols):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    test = ks_2samp(normal_rvs, standard_scaler(df[col]))\n",
    "    empirical_cumdist(normal_rvs, ax)\n",
    "    empirical_cumdist(standard_scaler(df[col]), ax, title=f\"{col}, p={round(test.pvalue, 2)}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datatest",
   "language": "python",
   "name": "datatest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
