{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# File Integrity\n",
    "\n",
    "The basic question is: \"Has the file changed since the last time you used it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hash (Browns)\n",
    "\n",
    "The layman definition of a hash: A fixed-length, scrambled string that uniquely identifies \"a thing\".\n",
    "\n",
    "The layman definition of a hashing function: A function that transforms \"a thing\" into a hash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `hashlib`\n",
    "\n",
    "Provides a library of hashing functions for hashing objects, strings, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import sha256, md5\n",
    "\n",
    "m = sha256()\n",
    "m.update('hello'.encode('utf-8'))\n",
    "m.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties of hashes & `hashlib` functions\n",
    "\n",
    "Hashes of the same \"thing\" should yield the same hash value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = sha256()\n",
    "m2.update('hello'.encode('utf-8'))\n",
    "m2.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar-looking but different strings will yield different hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3c48591d8d098a4538f5e013dfcf406e948eac4d3277b10bf614e295d6068179'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = sha256()\n",
    "m3.update('h√©llo'.encode('utf-8'))\n",
    "m3.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using a different hashing algorithm will yield a different hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5d41402abc4b2a76b9719d911017c592'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = md5()\n",
    "n.update('hello'.encode('utf-8'))\n",
    "n.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hashing functions don't work on all objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers cannot be hashed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    o = sha256()\n",
    "    o.update(3)\n",
    "except TypeError:\n",
    "    print('Numbers cannot be hashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings cannot be hashed without encoding.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    o = sha256()\n",
    "    o.update('Hello world!')\n",
    "except TypeError:\n",
    "    print('Strings cannot be hashed without encoding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0535e4be2b79ffd93291305436bf889314e4a3faec05ecffcbb7df31ad9e51a\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    o = sha256()\n",
    "    o.update('Hello world!'.encode('utf-8'))\n",
    "    print(o.hexdigest())\n",
    "except TypeError:\n",
    "    print('Strings must be encoded first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking for changes in data file\n",
    "\n",
    "Multiple approaches possible:\n",
    "\n",
    "- Check every cell against a \"master\" copy, assuming you have one. (**inefficient, but good for pinpointing tampered cells**)\n",
    "- Check every row against a hash of that row. (**somewhat inefficient, but good for practice, and good for pinpointing tampered rows**)\n",
    "- Check hash of a file. (**most efficient way**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise Part 1\n",
    "\n",
    "- Write a function that hashes strings and returns the digest, and add it to `datafuncs.py`. It should wrap the SHA256 algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_string(string):\n",
    "    \"\"\"\n",
    "    Convenience wrapper that returns the hash of a string.\n",
    "    \"\"\"\n",
    "    string = string.encode('utf-8')\n",
    "    return sha256(string).hexdigest()\n",
    "    \n",
    "hash_string('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise Part 2\n",
    "\n",
    "- Use `pandas` to open the data file, `data/Divvy_Stations_2013.csv` as the variable name `df`. \n",
    "- Create a new DataFrame called `hashes`.\n",
    "- Create a new column in `hashes` called `concat`, which is each column of data from `df` converted to strings and concatenated into a contiguous string.\n",
    "- Create a new column in `hashes` called `hash`, which is the computed the hash of each row of the contiguous strings.\n",
    "- Delete the `concat` column from `hashes`.\n",
    "- Save the hashes to disk as the file `hashes.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1aaf1b6343c89f1c7175fbad7f3ca1011fd9454da1169d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8a5215386187efb140c0050cea72d91b2a661d196d3176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8523ece8190a5049948d9219fe69a2dc48909da896a6c3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48983a013b5ad6c19cec8b3d02ec94139372e23fc1d1e0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6d3979c8ee2737e7cdce6dbe937d914688c973cb2d0ef5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hash\n",
       "0  1aaf1b6343c89f1c7175fbad7f3ca1011fd9454da1169d...\n",
       "1  8a5215386187efb140c0050cea72d91b2a661d196d3176...\n",
       "2  8523ece8190a5049948d9219fe69a2dc48909da896a6c3...\n",
       "3  48983a013b5ad6c19cec8b3d02ec94139372e23fc1d1e0...\n",
       "4  6d3979c8ee2737e7cdce6dbe937d914688c973cb2d0ef5..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Divvy_Stations_2013.csv')\n",
    "hashes = pd.DataFrame()  # don't modify the original dataframe.\n",
    "hashes['concat'] = df.apply(lambda x: ''.join(str(x[col]) for col in df.columns), axis=1)\n",
    "hashes['hash'] = hashes['concat'].apply(lambda x: hash_string(x))\n",
    "del hashes['concat']\n",
    "hashes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise Part 3\n",
    "\n",
    "- Now, wrap this in a function too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1aaf1b6343c89f1c7175fbad7f3ca1011fd9454da1169d...\n",
       "1    8a5215386187efb140c0050cea72d91b2a661d196d3176...\n",
       "2    8523ece8190a5049948d9219fe69a2dc48909da896a6c3...\n",
       "3    48983a013b5ad6c19cec8b3d02ec94139372e23fc1d1e0...\n",
       "4    6d3979c8ee2737e7cdce6dbe937d914688c973cb2d0ef5...\n",
       "Name: hash, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_data(dataframe):\n",
    "    hashes = pd.DataFrame()  # don't modify the original\n",
    "    hashes['concat'] = df.apply(lambda x: ''.join(str(x[col]) for col in df.columns), axis=1)\n",
    "    hashes['hash'] = hashes['concat'].apply(lambda x: hash_string(x))\n",
    "    del hashes['concat']\n",
    "    return hashes\n",
    "\n",
    "hash_data(df)['hash'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hash of a file\n",
    "\n",
    "It is possible to check the hash of a file. Let's add an existing implementation found online to our toolkit, `datafuncs.py`.\n",
    "\n",
    "(All credit to StackOverflow community: http://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hash_file(fname):\n",
    "    filehash = sha256()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            filehash.update(chunk)\n",
    "    return filehash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c861005089beb7f09e26a5b7afa09843a0ac1ca98fe9c36ac0510a58b21da40d'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_file('data/Divvy_Stations_2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'880ba1ef2e38e4c35df4b2cd745529797f08fb24048dea0600e8174518a99869'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_file('data/Divvy_Stations_2013_corrupt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "- Create a CSV file from a `pandas.DataFrame()` (or create a [`tinydb`](http://tinydb.readthedocs.io/en/latest/) database) to store the MD5 hashes of each file in the `data/` directory. Place this in the directory called `data_integrity/`. Be sure to record:\n",
    "    - File name.\n",
    "    - Hash.\n",
    "    - Date and time on which hash was computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tinydb import TinyDB, Query\n",
    "from datetime import datetime, date\n",
    "\n",
    "db = TinyDB('data_integrity/hashes.db')\n",
    "\n",
    "data_prefix = 'data'\n",
    "for f in os.listdir(data_prefix):\n",
    "    filehash = hash_file(f'{data_prefix}/{f}')\n",
    "    record = dict()\n",
    "    record['filename'] = f'{data_prefix}/{f}'\n",
    "    record['hash'] = filehash\n",
    "    record['datetime_hashed'] = datetime.today().isoformat()\n",
    "    db.insert(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "- Write a test that checks that the hash is the value that was recorded.\n",
    "- If you used a `tinydb` database, then check the API docs [here][tinydb] for more information on how to query for a particular record.\n",
    "\n",
    "[tinydb]: http://tinydb.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_file_hashes():\n",
    "    db = TinyDB('data_integrity/hashes.db')\n",
    "    \n",
    "    for f in os.listdir(data_prefix):\n",
    "        filename = f'{data_prefix}/{f}'\n",
    "        filehash = hash_file(filename)\n",
    "        Rec = Query()\n",
    "        latest_record = db.search(Rec.filename == filename)[-1]\n",
    "        assert latest_record['hash'] == filehash\n",
    "\n",
    "test_file_hashes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
